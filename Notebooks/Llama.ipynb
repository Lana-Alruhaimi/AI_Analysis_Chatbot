{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e08a1174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1451969",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai-community/gpt2\" #define model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a0e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Starting GPT Model Load \"openai/gpt-oss-20b\" in 4-bit ---\n"
     ]
    }
   ],
   "source": [
    "#loads model with 4 bit and saves bfloat16 for calculations\n",
    "bnb_config = BitsAndBytesConfig( #tells model how to compress and handle weights\n",
    "    load_in_4bit=True, #loads weights in 4 bit integer format (instead of 16 or 32 bits) to reduce memory footprint\n",
    "    bnb_4bit_quant_type=\"nf4\", #use normal 4 bit (n=normal, f=float, 4=4bit)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 #data used to compute after 4 bit weights are loaded and decompressed\n",
    ")\n",
    "print(f\"--- 1. Starting GPT 2 Model Load \\\"{model_id}\\\" in 4-bit ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: The model is quantized with Mxfp4Config but you are passing a BitsAndBytesConfig config. Please make sure to pass the same quantization config class to `from_pretrained` with different loading attributes.\n",
      "Ensure you have accepted the license on Hugging Face and your environment is active.\n"
     ]
    }
   ],
   "source": [
    "try: #goes to hugging face and download llama\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config, #passes 4 bit configuration\n",
    "        device_map=\"auto\", #manages device placement\n",
    "        #trust_remote_code=True #trust meta's Llama3.1 and excecute files\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    print(\"GPT 2 Model and Tokenizer loaded successfully.\")\n",
    "\n",
    "    prompt = \"Explain in one sentence why the sky is blue.\" #simple prompt to test chat\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    print(f\"\\n--- 2. Generating Simple Response for Prompt: '{prompt}' ---\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            **inputs, #unpacks dictionary\n",
    "            max_new_tokens=50, #set max length (50 because we are doing fast testing)\n",
    "            do_sample = False #false is highest probability (more predicatable), true would be based on distribution (more creative)\n",
    "        )\n",
    "        output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        print(\"\\n--- 3. Generation Complete ---\")\n",
    "        print(f\"Generated Output:\\n{output_text}\")\n",
    "        print(\"\\n Test complete. GPT 2 loading and generation are working.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(\"Ensure you have accepted the license on Hugging Face and your environment is active.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
