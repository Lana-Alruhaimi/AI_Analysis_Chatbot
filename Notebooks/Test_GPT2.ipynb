{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a1174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: THIS CODE IS A TEST TO SEE IF GPT 2 FROM HUGGING FACE IS WORKING WITH PYTHON AS INTENDED, AND IS NOT PART OF THE FINAL PRODUCT\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1451969",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai-community/gpt2\" #define model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "515a0e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Starting GPT 2 Model Load \"openai-community/gpt2\" in 4-bit ---\n"
     ]
    }
   ],
   "source": [
    "#loads model with 4 bit and saves bfloat16 for calculations\n",
    "bnb_config = BitsAndBytesConfig( #tells model how to compress and handle weights\n",
    "    load_in_4bit=True, #loads weights in 4 bit integer format (instead of 16 or 32 bits) to reduce memory footprint\n",
    "    bnb_4bit_quant_type=\"nf4\", #use normal 4 bit (n=normal, f=float, 4=4bit)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 #data used to compute after 4 bit weights are loaded and decompressed\n",
    ")\n",
    "print(f\"--- 1. Starting GPT 2 Model Load \\\"{model_id}\\\" in 4-bit ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362b318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0ce9fea5f848cebbd84e85da704e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lalruhaimi.t\\AppData\\Local\\anaconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lalruhaimi.t\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddeaf8586dd24a9e84b23a62f6af2f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67aa9512744144f3abdb9a226e041577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c907159c44428eb3ee7015fe842f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d7047b4fee4a1aaff8dbfe09769159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4292f41ec9ae45ff891b8334e2e79310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b22f36213d4e76ac2390e4c1c465c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT 2 Model and Tokenizer loaded successfully.\n",
      "\n",
      "--- 2. Generating Simple Response for Prompt: 'Explain in one sentence why the sky is blue.' ---\n",
      "\n",
      "--- 3. Generation Complete ---\n",
      "Generated Output:\n",
      "Explain in one sentence why the sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "\n",
      " Test complete. GPT 2 loading and generation are working.\n"
     ]
    }
   ],
   "source": [
    "try: #goes to hugging face and download GPT 2\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config, #passes 4 bit configuration\n",
    "        device_map=\"auto\", #manages device placement\n",
    "        #trust_remote_code=True #trust openAI's GPT2 and excecute files\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    print(\"GPT 2 Model and Tokenizer loaded successfully.\")\n",
    "\n",
    "    prompt = \"Explain in one sentence why the sky is blue.\" #simple prompt to test chat\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    print(f\"\\n--- 2. Generating Simple Response for Prompt: '{prompt}' ---\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            **inputs, #unpacks dictionary\n",
    "            max_new_tokens=50, #set max length (50 because we are doing fast testing)\n",
    "            do_sample = False #false is highest probability (more predicatable), true would be based on distribution (more creative)\n",
    "        )\n",
    "        output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        print(\"\\n--- 3. Generation Complete ---\")\n",
    "        print(f\"Generated Output:\\n{output_text}\")\n",
    "        print(\"\\n Test complete. GPT 2 loading and generation are working.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(\"Ensure you have accepted the license on Hugging Face and your environment is active.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
